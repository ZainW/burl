---
title: FAQ
description: Frequently asked questions about burl
---

## General

### What is burl?

burl is a modern HTTP benchmarking tool built with Bun. It features:
- Beautiful real-time terminal UI
- LLM-optimized output for AI integration
- Single binary distribution (no runtime dependencies)
- HTTP/1.1, HTTP/2, and experimental HTTP/3 support

### How is burl different from wrk, hey, or ab?

| Feature | burl | wrk | hey | ab |
|---------|------|-----|-----|-----|
| Installation | Single binary | Build from source | Go binary | Pre-installed |
| UI | Interactive TUI | Text only | Text only | Text only |
| LLM Output | ✅ | ❌ | ❌ | ❌ |
| Config Files | ✅ | Lua scripts | ❌ | ❌ |
| HTTP/2 | ✅ | ✅ | ✅ | ❌ |

Choose burl if you want modern developer experience, beautiful output, and AI integration.

### Is burl production-ready?

burl is actively developed and suitable for:
- ✅ Development and testing
- ✅ Load testing staging environments
- ✅ CI/CD performance testing
- ⚠️ Production load testing (use with caution, validate results)

Always validate benchmarking results against your specific use case.

### What is the license?

MIT-0 (MIT No Attribution). Use burl however you want, no attribution required.

## Installation

### Do I need Bun installed to use burl?

**No**. Pre-built binaries are standalone executables with no runtime dependencies.

You only need Bun if you're building from source.

### Can I install burl via npm or Homebrew?

Not yet, but it's planned. Current installation methods:
- Install script: `curl -fsSL https://raw.githubusercontent.com/ZainW/burl/master/install.sh | bash`
- Pre-built binaries from GitHub Releases
- Build from source with Bun

### Which binary should I download?

| Platform | Binary |
|----------|--------|
| macOS (Apple Silicon) | `burl-darwin-arm64` |
| macOS (Intel) | `burl-darwin-x64` |
| Linux (x64) | `burl-linux-x64` |
| Linux (ARM64) | `burl-linux-arm64` |
| Windows (x64) | `burl-windows-x64.exe` |

## Usage

### How do I benchmark an API endpoint?

Basic usage:

```bash
# Quick test
burl https://api.example.com/users

# Load test
burl https://api.example.com/users -c 50 -d 30s
```

### Can I use burl with POST/PUT requests?

Yes:

```bash
# POST with JSON
burl https://api.example.com/users \
  -m POST \
  -b '{"name":"test"}' \
  -T application/json

# PUT with file
burl https://api.example.com/users/123 \
  -m PUT \
  -B ./user.json \
  -T application/json
```

### How do I add authentication?

```bash
# Bearer token
burl https://api.example.com -a bearer:$TOKEN

# Basic auth
burl https://api.example.com -a basic:user:pass

# Custom header
burl https://api.example.com -H "Authorization: Custom token"
```

### Can I test localhost/local development servers?

Yes:

```bash
burl http://localhost:3000/api/users
burl http://127.0.0.1:8080/health
```

### How do I save results to a file?

```bash
# JSON output
burl https://api.example.com --llm json -o results.json

# CSV output
burl https://api.example.com -f csv -o results.csv

# Markdown output
burl https://api.example.com -f markdown -o results.md
```

## Configuration

### Can I save default options?

Yes, use a `.burlrc` config file:

```json
{
  "connections": 20,
  "duration": "30s",
  "http2": true
}
```

Place in your project directory or home directory (`~/.burlrc`).

### Do CLI arguments override config file values?

Yes, CLI arguments always take precedence over config file settings.

### Can I use environment variables in config?

Yes (planned feature):

```json
{
  "headers": {
    "Authorization": "Bearer ${API_TOKEN}"
  }
}
```

## Performance

### What's the maximum RPS burl can generate?

This depends on:
- Your hardware (CPU, network)
- Target server capacity
- Network latency
- Connection count and HTTP version

Typical ranges:
- Local server: 100K-500K RPS
- Remote server: 10K-50K RPS
- Limited by server, not burl

### Should I use HTTP/1.1 or HTTP/2?

**HTTP/2** is generally better for:
- Multiple concurrent requests
- Better multiplexing
- Lower latency

**HTTP/1.1** may be better for:
- Testing legacy systems
- Comparing against older tools
- Simple request/response patterns

Test both:

```bash
burl https://api.example.com --http1
burl https://api.example.com --http2
```

### How many connections should I use?

Start with **10-20 connections** and increase gradually:

```bash
# Light load
burl https://api.example.com -c 10

# Moderate load
burl https://api.example.com -c 50

# Heavy load
burl https://api.example.com -c 100
```

More connections ≠ better results. Find the sweet spot where:
- RPS plateaus
- Latency remains acceptable
- No connection errors

### What are warmup requests?

Warmup requests prime:
- JIT compilers
- Connection pools
- Caches
- Auto-scaling

Use warmup for more consistent results:

```bash
burl https://api.example.com -w 20 -d 30s
```

## Output & Formats

### What's LLM-optimized output?

LLM output includes:
- Structured JSON/Markdown
- Automatic performance interpretation
- Issue detection
- Optimization recommendations

Perfect for AI analysis:

```bash
burl https://api.example.com --llm json | \
  llm "What performance issues do you see?"
```

### How do I use burl in CI/CD?

```bash
# Generate JSON report
burl https://staging.api.com/health \
  --llm json \
  -o performance.json \
  --no-tui \
  --quiet

# Fail if P95 > 100ms
burl https://staging.api.com/health --llm json --no-tui | \
  jq -e '.latency_ms.p95 < 100'
```

### Can I disable the TUI?

Yes:

```bash
# Disable TUI
burl https://api.example.com --no-tui

# Minimal output
burl https://api.example.com --quiet

# Both
burl https://api.example.com --no-tui --quiet
```

### What do the percentiles mean?

- **P50 (median)**: 50% of requests completed faster
- **P90**: 90% of requests completed faster
- **P95**: 95% of requests completed faster
- **P99**: 99% of requests completed faster

P99 shows "worst case" latency for most users.

## Advanced

### What is latency correction?

Latency correction compensates for "coordinated omission" - when request scheduling delays hide true latency.

Use when:
- Target server is slow or inconsistent
- You need precise latency measurements
- Testing under sustained load

```bash
burl https://api.example.com --latency-correction
```

### Can I rate limit requests?

Yes, use QPS (queries per second):

```bash
# Limit to 100 requests/second
burl https://api.example.com -q 100 -d 30s
```

Useful for:
- Respecting API rate limits
- Gradual load ramping
- Realistic traffic simulation

### What's diagnostic mode?

Diagnostic mode shows timing breakdown:

```bash
burl https://api.example.com --diagnose
```

Displays:
- DNS lookup time
- TCP connection time
- TLS handshake time
- Time to first byte
- Total request time

Perfect for debugging slow connections.

### Can I benchmark GraphQL endpoints?

Yes:

```bash
burl https://api.example.com/graphql \
  -m POST \
  -b '{"query":"{ users { id name } }"}' \
  -T application/json \
  -c 20 -d 30s
```

### Can I test WebSocket endpoints?

Not currently. burl is designed for HTTP benchmarking only.

For WebSocket load testing, check out:
- [websocat](https://github.com/vi/websocat)
- [artillery](https://www.artillery.io/)

## Troubleshooting

### burl is slow compared to wrk/hey

1. **Check connections**: Try `-c 100` for higher concurrency
2. **Use HTTP/2**: Add `--http2` flag
3. **Disable TUI**: Use `--no-tui` for slightly better performance
4. **Compare fairly**: Ensure same conditions (connections, duration, HTTP version)

If still slower, please [open an issue](https://github.com/ZainW/burl/issues) with benchmark comparison.

### Results vary between runs

This is normal. To get more consistent results:

1. **Use warmup**: `-w 20`
2. **Increase duration**: `-d 60s` instead of `-d 10s`
3. **Run multiple times**: Average the results
4. **Isolate machine**: Close other applications

### Can I contribute to burl?

Yes! burl is open source. See [CONTRIBUTING.md](https://github.com/ZainW/burl/blob/master/CONTRIBUTING.md) for guidelines.

## Getting Help

**Documentation**: [burl.dev](https://burl.dev) (coming soon)
**GitHub Issues**: [github.com/ZainW/burl/issues](https://github.com/ZainW/burl/issues)
**Source Code**: [github.com/ZainW/burl](https://github.com/ZainW/burl)
